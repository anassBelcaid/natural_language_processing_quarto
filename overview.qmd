---
title: "Introduction to Natural Language Processing Course"
author: "A.Belcaid"
date: "09-01-2025"
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    code-fold: true
    fig-cap-location: bottom

#   pdf:
#     documentclass: article
#     geometry: margin=1in
#     number-sections: true
# execute:
echo: true
warning: false
bibliography: references.bib
# csl: IEEEtran.cls
---

# Course Overview {#sec-overview}

:::{.callout-note}
## Course Structure
- **Duration**: 14 weeks total
- **Format**: Student-led research presentations + practical sessions
- **Target**: Fifth-year computer science students
- **Prerequisites**: Machine Learning, Python programming, Linear Algebra
:::

```{mermaid}
%%| fig-cap: "Course Timeline and Structure"
%%| label: fig-timeline
gantt
    title Course Timeline
    dateFormat  YYYY-MM-DD
    section Topic Presentations
    Week 1 - NLP Fundamentals     :active, w1, 2024-09-01, 7d
    Week 2 - Language Models      :w2, after w1, 7d
    Week 3 - Word Embeddings      :w3, after w2, 7d
    Week 4 - Neural Networks      :w4, after w3, 7d
    Week 5 - Transformers         :w5, after w4, 7d
    Week 6 - Large LMs            :w6, after w5, 7d
    Week 7 - Ethics & Evaluation  :w7, after w6, 7d
    section Project Phase
    Project Development           :p1, after w7, 21d
    Final Presentations          :p2, after p1, 21d
```

**First 7 weeks**: Student presentations (2 hours) + Hands-on practice (1 hour)  
**Last 7 weeks**: Project presentations and peer evaluation

# Part I: Weekly Topics for Student Presentations {#sec-topics}

## Week 1: Fundamentals of Natural Language Processing {#sec-week1}

![NLP Pipeline Overview](https://media.geeksforgeeks.org/wp-content/uploads/20230118132334/NLP-Pipeline-GIF.gif){#fig-nlp-pipeline fig-alt="A flowchart showing the typical NLP processing pipeline from raw text to applications"}

### Topic Overview

- **History and Evolution**: From rule-based systems to modern neural approaches
- **Core Challenges**: Ambiguity, context, pragmatics, and world knowledge
- **Text Preprocessing**: Tokenization, normalization, and cleaning techniques
- **Linguistic Foundations**: Morphology, syntax, semantics, and pragmatics

:::{.callout-important}
## Key Resources
- [NLTK Book](https://www.nltk.org/book/) - Comprehensive introduction
- [spaCy Documentation](https://spacy.io/) - Industrial-strength NLP
- [Stanford NLP Group](https://nlp.stanford.edu/) - Research papers and resources
:::

### Learning Objectives

1. Understand the **scope and applications** of NLP across industries
2. Master the **text preprocessing pipeline** using industry-standard tools
3. Identify and analyze **linguistic ambiguities** and computational challenges
4. Implement **tokenization algorithms** for different languages and domains

### Hands-on Session

```python
# Sample preprocessing pipeline
import nltk
import spacy
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Text preprocessing demonstration
text = "The researchers are researching research papers."
# Expected output: tokens, stems, lemmas, POS tags
```

**Activities**:
- Building a custom tokenizer for social media text
- Comparing stemming vs. lemmatization performance
- Multilingual preprocessing challenges

## Week 2: Statistical Language Models and N-grams {#sec-week2}

### Topic Overview

Statistical language modeling forms the foundation of modern NLP. This week covers:

- **Probability Theory** in language: Chain rule, independence assumptions
- **N-gram Models**: Mathematical formulation and implementation details  
- **Smoothing Techniques**: Handling zero probabilities and data sparsity
- **Evaluation Metrics**: Perplexity, cross-entropy, and information theory

```{mermaid}
%%| fig-cap: "N-gram Model Hierarchy"
%%| label: fig-ngrams
graph TD
    # A[Unigram Model<br/>P(w)] --> B[Bigram Model<br/>P(w|w-1)]
    # B --> C[Trigram Model<br/>P(w|w-2,w-1)]
    # C --> D[N-gram Model<br/>P(w|w-n+1...w-1)]

    A --> E[Independence<br/>High bias, Low variance]
    D --> F[Context Dependence<br/>Low bias, High variance]
```

:::{.callout-tip}
## Mathematical Foundation
The n-gram probability is calculated as:
$$P(w_i|w_{i-n+1}^{i-1}) = \frac{C(w_{i-n+1}^{i})}{C(w_{i-n+1}^{i-1})}$$

Where $C(·)$ represents the count function in the training corpus.
:::

### Learning Objectives

- Build and evaluate n-gram language models from scratch
- Understand the **bias-variance tradeoff** in model complexity
- Apply various smoothing techniques (Laplace, Good-Turing, Kneser-Ney)
- Calculate and interpret perplexity scores

### Hands-on Session

**Implementation Tasks**:
- N-gram model training on different corpus sizes
- Perplexity calculation and analysis
- Smoothing method comparison study
- Text generation using trained models

## Week 3: Word Representations and Embeddings {#sec-week3}

![Word Embedding Space](https://via.placeholder.com/500x400/2196F3/FFFFFF?text=Word+Vectors+in+Semantic+Space:+King+−+Man+%2B+Woman+≈+Queen){#fig-embeddings fig-alt="Visualization of word embeddings showing semantic relationships in vector space"}

### Topic Overview

The transition from sparse to dense word representations revolutionized NLP:

- **Distributional Semantics**: "You shall know a word by the company it keeps"
- **Word2Vec Algorithms**: Skip-gram and Continuous Bag of Words (CBOW)
- **Global Vectors (GloVe)**: Matrix factorization approach to embeddings
- **FastText**: Subword information and out-of-vocabulary handling

:::{.callout-note}
## Word2Vec Intuition
- **Skip-gram**: Predicts context words given target word
- **CBOW**: Predicts target word given context words
- Both use hierarchical softmax or negative sampling for efficiency
:::

### Key Resources

- [Word2Vec Paper](https://arxiv.org/abs/1301.3781) - Original Mikolov et al. work
- [GloVe: Global Vectors](https://nlp.stanford.edu/projects/glove/) - Stanford implementation
- [FastText](https://fasttext.cc/) - Facebook's subword embeddings

### Learning Objectives

1. Understand limitations of **one-hot encoding** and sparse representations
2. Master the mathematics behind **Word2Vec training objectives**
3. Implement **embedding evaluation** using intrinsic and extrinsic methods
4. Analyze **semantic and syntactic** relationships in embedding spaces

### Hands-on Session

```python
# Word2Vec training example
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

# Training and visualization pipeline
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)
# Visualize embeddings with t-SNE
```

**Practical Exercises**:
- Training Word2Vec on domain-specific corpora
- t-SNE visualization of embedding clusters
- Word analogy tasks: "king - man + woman = ?"
- Cross-lingual embedding alignment

## Week 4: Neural Networks for NLP {#sec-week4}

### Topic Overview

The neural revolution in NLP began with architectures designed for sequential data:

- **Recurrent Neural Networks (RNNs)**: Processing variable-length sequences
- **Long Short-Term Memory (LSTM)**: Solving the vanishing gradient problem
- **Gated Recurrent Units (GRUs)**: Simplified gating mechanisms
- **Bidirectional Networks**: Capturing both forward and backward context

```{mermaid}
%%| fig-cap: "RNN Architecture Evolution"
%%| label: fig-rnn-evolution
graph LR
    A[Vanilla RNN<br/>Vanishing Gradients] --> B[LSTM<br/>Forget Gate + Input Gate + Output Gate]
    B --> C[GRU<br/>Reset Gate + Update Gate]
    C --> D[BiLSTM<br/>Forward + Backward Processing]
    
    B --> E[Applications:<br/>Language Modeling<br/>Sequence Classification<br/>Named Entity Recognition]
```

### Learning Objectives

- Design neural architectures for **sequence processing** tasks
- Understand **gradient flow** in recurrent connections
- Implement **bidirectional processing** for improved context modeling
- Apply **regularization techniques** specific to sequential data

### Hands-on Session

**Implementation Focus**:
- RNN-based language model implementation
- LSTM vs. GRU comparison on sequence classification
- Gradient clipping and other training stabilization techniques
- Attention visualization in sequence-to-sequence models

## Week 5: Attention Mechanisms and Transformers {#sec-week5}

![Transformer Architecture](https://via.placeholder.com/600x450/FF9800/FFFFFF?text=Transformer+Architecture:+Self-Attention+%2B+Feed-Forward+%2B+Residual+Connections){#fig-transformer fig-alt="Detailed diagram of the Transformer architecture showing encoder-decoder structure"}

### Topic Overview

The attention mechanism fundamentally changed how we process sequences:

- **Attention Intuition**: Differentiable dictionary lookup mechanism
- **Self-Attention**: Query, Key, Value matrices and scaled dot-product
- **Multi-Head Attention**: Parallel attention computations with different representations
- **Transformer Architecture**: Complete replacement of recurrence with attention

:::{.callout-important}
## Attention Formula
The scaled dot-product attention is computed as:
$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where $Q$, $K$, $V$ are query, key, and value matrices respectively.
:::

### Key Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer paper
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual explanation
- [Tensor2Tensor](https://github.com/tensorflow/tensor2tensor) - Reference implementation

### Learning Objectives

1. Grasp the **mathematical intuition** behind attention mechanisms
2. Understand **positional encoding** and why it's necessary
3. Implement **multi-head attention** from scratch
4. Analyze **computational complexity** advantages over RNNs

### Hands-on Session

```python
# Self-attention implementation
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        # Implementation details...
```

**Practical Tasks**:
- Implementing scaled dot-product attention
- Building a mini-Transformer for sequence classification
- Attention weight visualization and interpretation
- Positional encoding analysis

## Week 6: Large Language Models and Pre-training {#sec-week6}

### Topic Overview

The era of large-scale pre-trained models has transformed NLP applications:

- **Pre-training Paradigms**: From word prediction to masked language modeling
- **BERT Family**: Bidirectional encoder representations from transformers
- **GPT Series**: Autoregressive language model scaling
- **Transfer Learning**: Fine-tuning strategies for downstream tasks

```{mermaid}
%%| fig-cap: "Evolution of Large Language Models"
%%| label: fig-llm-timeline
timeline
    title LLM Evolution Timeline
    2018 : BERT (Google)
         : GPT-1 (OpenAI)
         : ELMo (AllenNLP)
    2019 : GPT-2
         : RoBERTa
         : XLNet
    2020 : GPT-3
         : T5
         : ELECTRA
    2021 : PaLM
         : Codex
         : WebGPT
    2022 : ChatGPT
         : InstructGPT
         : LaMDA
    2023 : GPT-4
         : Claude
         : LLaMA
```

:::{.callout-note}
## Pre-training Objectives
- **Masked Language Modeling (MLM)**: Predict masked tokens (BERT)
- **Causal Language Modeling (CLM)**: Predict next token (GPT)
- **Prefix Language Modeling**: Hybrid approach (GLM, PaLM)
:::

### Learning Objectives

- Understand different **pre-training strategies** and their trade-offs
- Master **transfer learning** concepts for NLP applications
- Analyze **scaling laws** and emergent capabilities in large models
- Implement **fine-tuning pipelines** for specific tasks

### Hands-on Session

**Practical Applications**:
- Fine-tuning BERT for text classification using [Hugging Face](https://huggingface.co/)
- Prompt engineering with GPT models
- Model comparison across different architectures
- Efficient fine-tuning with parameter-efficient methods (LoRA, adapters)

## Week 7: Evaluation and Ethics in NLP {#sec-week7}

### Topic Overview

Responsible AI development requires comprehensive evaluation and ethical considerations:

- **Evaluation Methodologies**: Intrinsic vs. extrinsic evaluation frameworks
- **Bias Detection**: Identifying and measuring algorithmic bias
- **Fairness Metrics**: Demographic parity, equalized odds, individual fairness
- **Environmental Impact**: Carbon footprint of large model training

![Bias in NLP Systems](https://via.placeholder.com/550x350/F44336/FFFFFF?text=Bias+Sources:+Data+%2B+Algorithms+%2B+Applications+%3D+Societal+Impact){#fig-bias fig-alt="Diagram showing how bias enters NLP systems through data, algorithms, and applications"}

:::{.callout-warning}
## Ethical Considerations
- **Representation Bias**: Training data may not represent all populations
- **Measurement Bias**: Evaluation metrics may favor certain groups
- **Evaluation Bias**: Test sets may contain demographic skews
- **Deployment Bias**: Real-world usage may differ from intended applications
:::

### Key Resources

- [AI Fairness 360](https://aif360.readthedocs.io/) - IBM's bias detection toolkit
- [What We Can't Measure, We Can't Understand](https://arxiv.org/abs/2109.09315) - Measurement in ML
- [Blodgett et al., 2020](https://arxiv.org/abs/2005.14050) - Language models and bias

### Learning Objectives

1. Design **comprehensive evaluation frameworks** for NLP systems
2. Identify and **quantify algorithmic bias** using statistical methods
3. Implement **bias mitigation techniques** at different pipeline stages
4. Consider **broader societal implications** of NLP deployment

### Hands-on Session

```python
# Bias evaluation example
from sklearn.metrics import confusion_matrix
import pandas as pd

# Fairness metric calculation
def demographic_parity_difference(y_true, y_pred, sensitive_attr):
    # Implementation for measuring bias across groups
    pass
```

**Evaluation Tasks**:
- Word embedding bias testing (WEAT, SEAT)
- Fairness evaluation across demographic groups
- Carbon footprint estimation for model training
- Adversarial testing for robustness

# Part II: Student Project Presentations {#sec-projects}

## Project Categories and Detailed Descriptions

### Category A: Text Classification and Analysis {#sec-classification}

#### Project 1: Multi-label News Article Classification {#sec-project1}

**Objective**: Build a robust multi-label classification system for news articles

- **Dataset**: [Reuters-21578](http://www.daviddlewis.com/resources/testcollections/reuters21578/) or [BBC News Dataset](https://www.kaggle.com/c/learn-ai-bbc)
- **Techniques**: Compare traditional ML (TF-IDF + SVM) vs. modern approaches (BERT, RoBERTa)
- **Evaluation**: Multi-label F1, Hamming loss, subset accuracy
- **Extensions**: Hierarchical classification, active learning for label efficiency

:::{.callout-tip}
## Technical Challenges
- **Label Imbalance**: Some categories have very few examples
- **Label Correlation**: Economic news often overlaps with political news
- **Temporal Drift**: News topics evolve over time
:::

#### Project 2: Aspect-Based Sentiment Analysis {#sec-project2}

**Objective**: Joint extraction of aspects and sentiment classification

- **Dataset**: [SemEval ABSA datasets](http://alt.qcri.org/semeval2016/task5/), restaurant/hotel reviews
- **Techniques**: Joint models for aspect extraction and sentiment classification
- **Evaluation**: Aspect-level F1 scores, sentiment accuracy per aspect
- **Extensions**: Cross-domain adaptation, multilingual ABSA

```python
# Example aspect-sentiment pairs
text = "The pizza was delicious but the service was terrible."
aspects = [
    ("pizza", "positive"),
    ("service", "negative")
]
```

#### Project 3: Fake News Detection with Explainability {#sec-project3}

**Objective**: Build interpretable fake news detection systems

- **Dataset**: [FakeNewsNet](https://github.com/KaiDMML/FakeNewsNet), [LIAR dataset](https://www.cs.ucsb.edu/~william/data/liar_dataset.zip)
- **Techniques**: Feature engineering + deep learning + attention visualization
- **Evaluation**: Classification metrics + human evaluation of explanations
- **Extensions**: Multi-modal fake news detection (text + images)

### Category B: Information Extraction and Retrieval {#sec-extraction}

#### Project 4: Named Entity Recognition for Specialized Domains {#sec-project4}

**Objective**: Develop domain-specific NER systems

- **Dataset**: [BioBERT datasets](https://github.com/dmis-lab/biobert) (biomedical) or financial NER
- **Techniques**: CRF vs. BERT-based sequence labeling
- **Evaluation**: Entity-level F1, error analysis by entity type
- **Extensions**: Few-shot NER, nested entity recognition

#### Project 5: Open-Domain Question Answering System {#sec-project5}

**Objective**: Build end-to-end question answering pipeline

- **Dataset**: [Natural Questions](https://ai.google.com/research/NaturalQuestions/), [MS MARCO](https://microsoft.github.io/msmarco/)
- **Architecture**: Dense Passage Retrieval + Reading Comprehension
- **Evaluation**: Exact match, F1, answer coverage analysis
- **Extensions**: Multi-hop reasoning, conversational QA

```{mermaid}
%%| fig-cap: "Question Answering System Architecture"
%%| label: fig-qa-system
graph TB
    A[Question] --> B[Query Encoder]
    B --> C[Passage Retrieval]
    C --> D[Passage Encoder]
    D --> E[Answer Extraction]
    E --> F[Answer]
    
    G[(Knowledge Base<br/>Wikipedia/Web)] --> C
    H[Retriever Model<br/>DPR/ColBERT] --> C
    I[Reader Model<br/>BERT/T5] --> E
```

#### Project 6: Automatic Fact Verification {#sec-project6}

**Objective**: Verify claims against evidence sources

- **Dataset**: [FEVER](https://fever.ai/dataset/fever.html) (Fact Extraction and VERification)
- **Pipeline**: Evidence retrieval → Textual entailment → Verdict prediction
- **Evaluation**: FEVER score, evidence selection accuracy
- **Extensions**: Real-time fact-checking, claim generation

### Category C: Text Generation and Summarization {#sec-generation}

#### Project 7: Neural Abstractive Text Summarization {#sec-project7}

**Objective**: Generate coherent abstractive summaries

- **Dataset**: [CNN/DailyMail](https://github.com/abisee/cnn-dailymail), [XSum](https://github.com/EdinburghNLP/XSum)
- **Techniques**: Encoder-decoder with attention, copy mechanisms, coverage
- **Evaluation**: ROUGE scores, BERTScore, factual consistency
- **Extensions**: Multi-document summarization, controllable length

#### Project 8: Dialogue System with Personality {#sec-project8}

**Objective**: Develop persona-consistent chatbots

- **Dataset**: [PersonaChat](https://github.com/facebookresearch/ParlAI/tree/main/projects/personachat), [Blended Skill Talk](https://github.com/facebookresearch/ParlAI/tree/main/projects/blended_skill_talk)
- **Techniques**: Persona-aware generation, retrieval-augmented responses
- **Evaluation**: Automatic metrics + human evaluation for consistency
- **Extensions**: Emotional intelligence, long-term memory

#### Project 9: Creative Writing Assistant {#sec-project9}

**Objective**: AI-powered creative content generation

- **Dataset**: [WritingPrompts](https://www.reddit.com/r/WritingPrompts/), poetry corpora
- **Techniques**: Fine-tuned GPT models with controlled generation
- **Evaluation**: Creativity metrics, human preference studies, style analysis
- **Extensions**: Interactive story writing, genre transfer

### Category D: Multilingual and Low-Resource NLP {#sec-multilingual}

#### Project 10: Cross-lingual Text Classification {#sec-project10}

**Objective**: Zero-shot transfer across languages

- **Dataset**: [MLDoc](https://github.com/facebookresearch/MLDoc), [XNLI](https://github.com/facebookresearch/XNLI)
- **Techniques**: Multilingual BERT, cross-lingual word embeddings
- **Evaluation**: Zero-shot transfer performance analysis
- **Extensions**: Few-shot adaptation, code-switching handling

#### Project 11: Machine Translation for Low-Resource Languages {#sec-project11}

**Objective**: Improve translation for under-resourced languages

- **Dataset**: [OPUS collections](http://opus.nlpl.eu/), WMT shared tasks
- **Techniques**: Transfer learning, back-translation, multilingual models
- **Evaluation**: BLEU, chrF, human evaluation, error analysis
- **Extensions**: Pivot translation, unsupervised MT

### Category E: Specialized Applications {#sec-specialized}

#### Project 12: Legal Document Analysis {#sec-project12}

**Objective**: Automated legal document processing

- **Dataset**: Legal case documents, contracts, legislative texts
- **Techniques**: Domain-adapted BERT, hierarchical document modeling
- **Evaluation**: Legal expert evaluation, domain-specific metrics
- **Extensions**: Legal precedent search, contract risk assessment

#### Project 13: Medical Text Mining {#sec-project13}

**Objective**: Clinical decision support through NLP

- **Dataset**: [MIMIC-III](https://mimic.mit.edu/) clinical notes, PubMed abstracts
- **Techniques**: BioBERT, clinical NER, relation extraction
- **Evaluation**: Medical accuracy, clinical utility assessment
- **Extensions**: Drug interaction prediction, diagnosis assistance

#### Project 14: Mental Health Detection in Social Media {#sec-project14}

**Objective**: Early detection of mental health indicators

- **Dataset**: Reddit mental health datasets, Twitter emotion data
- **Techniques**: Multi-modal analysis, temporal pattern modeling
- **Evaluation**: Precision/recall for sensitive detection, ethical review
- **Extensions**: Crisis intervention systems, privacy-preserving methods

:::{.callout-warning}
## Ethical Considerations for Mental Health Projects
- **Privacy**: Anonymization and consent requirements
- **Harm Prevention**: Avoiding false positives in crisis detection
- **Professional Oversight**: Collaboration with mental health professionals
- **Bias Mitigation**: Ensuring fair representation across demographics
:::

# Assessment Criteria {#sec-assessment}

## Topic Presentations (Weeks 1-7) {#sec-topic-assessment}

| Criterion | Weight | Description |
|-----------|--------|-------------|
| **Content Quality** | 40% | Research depth, technical accuracy, concept coverage |
| **Presentation Skills** | 30% | Communication clarity, visual aids, audience engagement |
| **Technical Implementation** | 30% | Code quality, demonstration effectiveness, innovation |

: Assessment rubric for weekly topic presentations {#tbl-topic-rubric}

## Project Presentations (Weeks 8-14) {#sec-project-assessment}

| Criterion | Weight | Key Evaluation Points |
|-----------|--------|---------------------|
| **Technical Rigor** | 35% | Methodology selection, implementation quality, experimental design |
| **Innovation** | 25% | Novel approaches, creative problem-solving, original insights |
| **Results & Analysis** | 25% | Comprehensive evaluation, error analysis, baseline comparison |
| **Communication** | 15% | Clear presentation, professional documentation, visualizations |

: Assessment rubric for final project presentations {#tbl-project-rubric}

:::{.callout-note}
## Peer Evaluation Component
Students will also evaluate their peers' presentations using a structured rubric, contributing 10% to the final grade. This encourages active engagement and critical thinking.
:::

# Resources and Tools {#sec-resources}

## Essential Python Libraries

```python
# Core NLP libraries
import nltk              # Natural Language Toolkit
import spacy             # Industrial-strength NLP
import transformers      # Hugging Face Transformers
import torch            # PyTorch for deep learning
import tensorflow as tf # TensorFlow alternative

# Specialized libraries
import gensim           # Topic modeling and word embeddings
import scikit-learn     # Traditional ML algorithms
import datasets         # Hugging Face datasets
import evaluate         # Evaluation metrics
```

## Dataset Repositories

- **[Hugging Face Datasets](https://huggingface.co/datasets)**: 10,000+ datasets for NLP
- **[Papers with Code](https://paperswithcode.com/datasets)**: Benchmark datasets with leaderboards
- **[Google Dataset Search](https://datasetsearch.research.google.com/)**: Comprehensive dataset discovery
- **[Academic Torrents](https://academictorrents.com/)**: Large-scale research datasets

## Evaluation Tools and Metrics

- **Text Generation**: BLEU, ROUGE, BERTScore, METEOR
- **Classification**: Precision, Recall, F1, AUC-ROC
- **Sequence Labeling**: seqeval, entity-level F1
- **Bias and Fairness**: Fairlearn, AI Fairness 360

# Timeline and Milestones {#sec-timeline}

```{mermaid}
%%| fig-cap: "Detailed Course Timeline with Milestones"
%%| label: fig-detailed-timeline
gantt
    title Detailed Course Timeline
    dateFormat  YYYY-MM-DD
    section Topic Presentations
    Week 1: NLP Fundamentals      :active, t1, 2024-09-01, 7d
    Week 2: Language Models       :t2, after t1, 7d
    Week 3: Word Embeddings       :t3, after t2, 7d
    Week 4: Neural Networks       :t4, after t3, 7d
    Week 5: Transformers          :t5, after t4, 7d
    Week 6: Large Language Models :t6, after t5, 7d
    Week 7: Ethics & Evaluation   :t7, after t6, 7d
    section Milestones
    Project Proposal Due          :milestone, m1, 2024-10-01, 0d
    Progress Check Meeting        :milestone, m2, 2024-11-01, 0d
    Final Presentation           :milestone, m3, 2024-12-01, 0d
    section Project Development
    Project Phase 1              :p1, after t7, 21d
    Project Phase 2              :p2, after p1, 21d
    Final Presentations          :p3, after p2, 14d
```

## Key Milestones

:::{.panel-tabset}

### Week 4: Mid-Course Checkpoint
- **Project proposal submission** (2-page document)
- **Literature review** progress check
- **Dataset acquisition** and preliminary analysis
- **Technical approach** validation with instructor

### Week 7: Proposal Presentations
- **5-minute project pitches** to class
- **Peer feedback** and suggestions
- **Final project scope** confirmation
- **Team formation** (if applicable)

### Week 10: Progress Review
- **Interim results** presentation
- **Technical challenges** discussion
- **Methodology adjustments** if needed
- **Timeline reassessment** and planning

### Week 13-14: Final Presentations
- **15-minute presentations** with Q&A
- **Live demonstrations** of working systems
- **Peer evaluation** and feedback
- **Industry guest evaluators** (when possible)

:::

# Additional Course Information {#sec-additional}

## Collaboration and Academic Integrity

- **Individual Work**: Weekly topic presentations must be completed individually
- **Team Projects**: Final projects may be completed in teams of maximum 2 students
- **Code Sharing**: All implementations must be original with proper attribution
- **Plagiarism Policy**: Zero tolerance for academic dishonesty

## Technical Requirements

:::{.callout-important}
## Submission Requirements
- **Version Control**: All projects must use Git with clear commit history
- **Reproducibility**: Include requirements.txt and detailed setup instructions
- **Documentation**: README with project description, usage, and results
- **Public Repository**: GitHub repository with appropriate license
:::

## Support and Office Hours

- **Weekly Office Hours**: Tuesdays 2-4 PM and Thursdays 10-12 PM
- **Online Forum**: Course Slack workspace for peer discussion
- **Technical Support**: TA sessions for implementation help
- **Guest Speakers**: Industry professionals and researchers (select weeks)

## Computing Resources

Students have access to:
- **Local GPUs**: NVIDIA RTX 4090 for model training
- **Cloud Credits**: $100 Google Cloud Platform credits per student
- **Cluster Access**: University HPC cluster for large-scale experiments
- **Pretrained Models**: Access to Hugging Face Pro for latest models

---

*This course syllabus is subject to updates based on student needs and emerging trends in NLP research. All changes will be communicated through the course management system.*
